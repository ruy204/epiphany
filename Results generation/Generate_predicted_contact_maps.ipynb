{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Step1_generate_predicted_contact_maps",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: generate contact maps using well-trained model \n"
      ],
      "metadata": {
        "id": "txRTWgCo0gDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load packages"
      ],
      "metadata": {
        "id": "sRpiz2qr0l7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDRLbhxMzvGa",
        "outputId": "511896fd-ed7e-4c3a-8c7f-2fea2afb4c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hickle in /usr/local/lib/python3.7/dist-packages (4.0.4)\n",
            "Requirement already satisfied: h5py<3.0.0,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from hickle) (2.10.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from hickle) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from hickle) (1.19.5)\n",
            "Requirement already satisfied: dill>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from hickle) (0.3.4)\n",
            "Requirement already satisfied: pyBigWig in /usr/local/lib/python3.7/dist-packages (0.3.18)\n",
            "--2021-12-16 20:53:31--  https://s3.amazonaws.com/hicfiles.tc4ga.com/public/juicer/juicer_tools_1.22.01.jar\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.186.141\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.186.141|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36188666 (35M) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘juicer_tools_1.22.01.jar’\n",
            "\n",
            "juicer_tools_1.22.0 100%[===================>]  34.51M  30.3MB/s    in 1.1s    \n",
            "\n",
            "2021-12-16 20:53:32 (30.3 MB/s) - ‘juicer_tools_1.22.01.jar’ saved [36188666/36188666]\n",
            "\n",
            "--2021-12-16 20:53:32--  http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/genes/hg38.refGene.gtf.gz\n",
            "Resolving hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)... 128.114.119.163\n",
            "Connecting to hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)|128.114.119.163|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23614606 (23M) [application/x-gzip]\n",
            "Saving to: ‘hg38.refGene.gtf.gz’\n",
            "\n",
            "hg38.refGene.gtf.gz 100%[===================>]  22.52M  6.63MB/s    in 3.4s    \n",
            "\n",
            "2021-12-16 20:53:36 (6.63 MB/s) - ‘hg38.refGene.gtf.gz’ saved [23614606/23614606]\n",
            "\n",
            "gzip: /content/hg38.refGene.gtf.gz: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "##########################\n",
        "#    Loading packages    #\n",
        "##########################\n",
        "\n",
        "# 1. Load packages\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch import randn\n",
        "from torch.nn import MSELoss\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "!pip install hickle\n",
        "import hickle as hkl\n",
        "from torch.autograd import Variable\n",
        "import gzip\n",
        "import sys\n",
        "import os \n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "!pip install pyBigWig\n",
        "import pyBigWig\n",
        "\n",
        "# 2. Load data - part 2\n",
        "\n",
        "!wget https://s3.amazonaws.com/hicfiles.tc4ga.com/public/juicer/juicer_tools_1.22.01.jar\n",
        "!wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/genes/hg38.refGene.gtf.gz\n",
        "!gunzip /content/hg38.refGene.gtf.gz\n",
        "\n",
        "chrom_list = [\"chr\"+str(i) for i in range(1,23)] #for human hg38\n",
        "length_list = [248956422,242193529,198295559,190214555,181538259,170805979,159345973,145138636,\n",
        "               138394717,133797422,135086622,133275309,114364328,107043718,101991189,90338345,\n",
        "               83257441,80373285,58617616,64444167,46709983,50818468]\n",
        "chrom_len_dict = dict(zip(chrom_list,length_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load trained model "
      ],
      "metadata": {
        "id": "8GZHrNoG0cUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Research/Predict2D/notebooks5/util_functions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84A3wXbd1zkC",
        "outputId": "a699593f-e58e-4ff4-e83c-ec6e56fc3d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research/Predict2D/notebooks5/util_functions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "#    Loading trained model    #\n",
        "###############################\n",
        "\n",
        "#1. import model architecture and util functions \n",
        "\n",
        "from model_architecture_util import *\n",
        "\n",
        "#2. load model \n",
        "\n",
        "wsize = 14000\n",
        "net = Net(window_size=wsize)\n",
        "restore(net,'/content/trained_model')\n",
        "net.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE4KWTM73A1f",
        "outputId": "f5a76ab3-bec4-4158-c5b5-8c8021955cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restoring:\n",
            "conv1.conv.weight -> \ttorch.Size([70, 5, 17]) = 0MB\n",
            "conv1.conv.bias -> \ttorch.Size([70]) = 0MB\n",
            "conv2.conv.weight -> \ttorch.Size([90, 70, 7]) = 0MB\n",
            "conv2.conv.bias -> \ttorch.Size([90]) = 0MB\n",
            "conv3.conv.weight -> \ttorch.Size([70, 90, 5]) = 0MB\n",
            "conv3.conv.bias -> \ttorch.Size([70]) = 0MB\n",
            "conv4.conv.weight -> \ttorch.Size([20, 70, 5]) = 0MB\n",
            "conv4.conv.bias -> \ttorch.Size([20]) = 0MB\n",
            "rnn1.weight_ih_l0 -> \ttorch.Size([4800, 900]) = 17MB\n",
            "rnn1.weight_hh_l0 -> \ttorch.Size([4800, 1200]) = 23MB\n",
            "rnn1.bias_ih_l0 -> \ttorch.Size([4800]) = 0MB\n",
            "rnn1.bias_hh_l0 -> \ttorch.Size([4800]) = 0MB\n",
            "rnn1.weight_ih_l0_reverse -> \ttorch.Size([4800, 900]) = 17MB\n",
            "rnn1.weight_hh_l0_reverse -> \ttorch.Size([4800, 1200]) = 23MB\n",
            "rnn1.bias_ih_l0_reverse -> \ttorch.Size([4800]) = 0MB\n",
            "rnn1.bias_hh_l0_reverse -> \ttorch.Size([4800]) = 0MB\n",
            "rnn2.weight_ih_l0 -> \ttorch.Size([4800, 2400]) = 46MB\n",
            "rnn2.weight_hh_l0 -> \ttorch.Size([4800, 1200]) = 23MB\n",
            "rnn2.bias_ih_l0 -> \ttorch.Size([4800]) = 0MB\n",
            "rnn2.bias_hh_l0 -> \ttorch.Size([4800]) = 0MB\n",
            "rnn2.weight_ih_l0_reverse -> \ttorch.Size([4800, 2400]) = 46MB\n",
            "rnn2.weight_hh_l0_reverse -> \ttorch.Size([4800, 1200]) = 23MB\n",
            "rnn2.bias_ih_l0_reverse -> \ttorch.Size([4800]) = 0MB\n",
            "rnn2.bias_hh_l0_reverse -> \ttorch.Size([4800]) = 0MB\n",
            "rnn3.weight_ih_l0 -> \ttorch.Size([4800, 2400]) = 46MB\n",
            "rnn3.weight_hh_l0 -> \ttorch.Size([4800, 1200]) = 23MB\n",
            "rnn3.bias_ih_l0 -> \ttorch.Size([4800]) = 0MB\n",
            "rnn3.bias_hh_l0 -> \ttorch.Size([4800]) = 0MB\n",
            "rnn3.weight_ih_l0_reverse -> \ttorch.Size([4800, 2400]) = 46MB\n",
            "rnn3.weight_hh_l0_reverse -> \ttorch.Size([4800, 1200]) = 23MB\n",
            "rnn3.bias_ih_l0_reverse -> \ttorch.Size([4800]) = 0MB\n",
            "rnn3.bias_hh_l0_reverse -> \ttorch.Size([4800]) = 0MB\n",
            "fc.weight -> \ttorch.Size([900, 2400]) = 8MB\n",
            "fc.bias -> \ttorch.Size([900]) = 0MB\n",
            "fc2.weight -> \ttorch.Size([100, 900]) = 0MB\n",
            "fc2.bias -> \ttorch.Size([100]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored /content/drive/MyDrive/Research/Predict2D/latest models/I/054.pt_model\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): ConvBlock(\n",
              "    (conv): Conv1d(5, 70, kernel_size=(17,), stride=(1,))\n",
              "    (act): ReLU()\n",
              "    (pool): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (do1): Dropout(p=0.1, inplace=False)\n",
              "  (conv2): ConvBlock(\n",
              "    (conv): Conv1d(70, 90, kernel_size=(7,), stride=(1,))\n",
              "    (act): ReLU()\n",
              "    (pool): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (do2): Dropout(p=0.1, inplace=False)\n",
              "  (conv3): ConvBlock(\n",
              "    (conv): Conv1d(90, 70, kernel_size=(5,), stride=(1,))\n",
              "    (act): ReLU()\n",
              "    (pool): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (do3): Dropout(p=0.1, inplace=False)\n",
              "  (conv4): ConvBlock(\n",
              "    (conv): Conv1d(70, 20, kernel_size=(5,), stride=(1,))\n",
              "    (act): ReLU()\n",
              "  )\n",
              "  (pool): AdaptiveMaxPool1d(output_size=45)\n",
              "  (do4): Dropout(p=0.1, inplace=False)\n",
              "  (rnn1): LSTM(900, 1200, batch_first=True, bidirectional=True)\n",
              "  (rnn2): LSTM(2400, 1200, batch_first=True, bidirectional=True)\n",
              "  (rnn3): LSTM(2400, 1200, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=2400, out_features=900, bias=True)\n",
              "  (act): ReLU()\n",
              "  (fc2): Linear(in_features=900, out_features=100, bias=True)\n",
              "  (act2): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generating predictions using trained model \n",
        "\n",
        "Please "
      ],
      "metadata": {
        "id": "0wnh279v0STH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################\n",
        "#    Generating predictions using trained model    #\n",
        "####################################################\n",
        "\n",
        "#1. import util functions\n",
        "\n",
        "from generate_predictions_util import *\n",
        "\n",
        "#2. generate predictions for chromosomes\n",
        "\n",
        "pth = '/content/Ground_truth_HiC_diagonal_list.txt'\n",
        "for i in [1,23]:\n",
        "  chrom = \"chr\" + str(i)\n",
        "  print(chrom,datetime.now())\n",
        "  results_generation(chrom = chrom,\n",
        "                     cell_type = \"GM12878\", \n",
        "                     bwfile_dir = \"/content/bigWig/GRCh38\",\n",
        "                     submatrix_location = \"/content/loc1.txt\", \n",
        "                     assemble_matrix_location = \"/content/loc2.txt\",\n",
        "                     ground_truth_file = pth, \n",
        "                     ground_truth_location = \"/content/loc3.txt\", \n",
        "                     window_size = wsize) #normcounts, zvalue, zfull"
      ],
      "metadata": {
        "id": "-ImDhphNz_lq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}